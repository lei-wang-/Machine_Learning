---
title: "Machine Learning"
author: ''
date: "Sunday, June 22, 2014"
output: html_document
---

This report studies the qualitative activity recognition of weight lifting exercise. Several machine learning models (generalized boosting model, support vector machine, and random forest) are built to predict the manner the subjectives do their weight lifting. The 10-fold cross-validation method is used to learn the model parameters and also evaluate the out of sample error of different models. At last, The best one is selected to predict 20 different 
test cases.


### Preprocess the data

There're many features in the weight lifting exercise dataset which lacks variance, either of which is blank or NA value. They are not useful for our prediction analysis, hence removing them to get clean data.

```{r, cache=TRUE}
data <- read.csv("./pml-training.csv")
testData <- read.csv("./pml-testing.csv")
library(caret)
filter1 <- function(data){
    temp <- data[data$new_window == "no" , ]
    index <- nearZeroVar(temp, saveMetrics = T)$nzv
    temp[ , !index]
}
cleanTestData <- filter1(testData)[ , -c(1,2,3,4,5,6)]
cleanData <- filter1(data)[ , -c(1,2,3,4,5,6)]
dim(cleanData)
```

The cleanData set has 53 variables, of which 52 are features relevant to weight lifting exercise and the last one is the outcome or the manner that subjective did the exercise. We'll build several prediction models based on those 52 features. Those 52 features are probably correlated with each other. We use the Principal Component Analysis (PCA) method to get a set of uncorrelated features, and select those with high data variance to further shrink our feature basis set.

```{r}
analysisData <- prcomp(cleanData[ , -53] , scale. = TRUE)
varianceData <- analysisData$sdev^2
relative <- varianceData/sum(varianceData)
cumsum(relative)
```

We can see that the first 12 principal components can capture 80% of variance of the data, and 36 principal components can capture 99% of data variance. In this report, We use 36 components of PCA method. 


### Study Design and Model prediction

The generalized boosting model, support vector machine, and random forest are studied by using 10-fold cross validation.

```{r, cache=TRUE}
library(caret)
library(gbm)
preProcValue <- preProcess(cleanData[, -53], method = "pca", pcaComp = 36)
trainData <- predict(preProcValue, cleanData[, -53])
fitControl <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 1)
gbmGrid <- expand.grid(interaction.depth = c(1, 3, 5, 7),
                       n.trees = (1:3)*50,
                       shrinkage = 0.1)
set.seed(825)
gbmFit <- train(cleanData$classe ~ ., data = trainData,
               method = "gbm",
               trControl = fitControl,
               tuneGrid = gbmGrid,
               verbose = FALSE)
gbmFit
```
```{r}
library(ggplot2)
ggplot(gbmFit)
```

Three essential parameters needs to tune to get a robust boosting model. Based on the graph, the number of ensembe trees for boosting is 150. The interaction depth for the tree is 7. The shrinkage value is 0.1 . The expected out of sample error can be determined by accuracy, which is 10%.



```{r, cache=TRUE}
library(kernlab)
set.seed(1056)
svmFit <- train(cleanData$classe ~ ., data = trainData,
                method = "svmRadial",
                tuneLength = 10,
                trControl = fitControl
                )
svmFit
```
```{r}
ggplot(svmFit)
```

The variance-bias tradeoff value is an important value to tune the SVM. Here, the tuneLength is 10, which means the tradeoff value runs from 2^-2 to 2^8. The last value 128 is selected. The gaussian kernel is used here. The standard error for this kernel uses value 0.02 . The expected out of sample error is very small, approaching 0 .


```{r, cache=TRUE}
library(randomForest)
set.seed(1410)
rfFit <- train(cleanData$classe ~ ., data = trainData,
               method = "rf",
               trControl = fitControl,
               ntree = 1000,
               importance = TRUE)
rfFit
```
```{r}
ggplot(rfFit)
```

The number of trees for random forest algorithm is 1000. The number of features randomly selected at each split is 2. The expected out of sample error is 0 or very small.

```{r}
library(caret)
resamps <- resamples(list(GBM=gbmFit, SVM=svmFit, RF=rfFit))
resamps
bwplot(resamps, layout=c(3,1))
```

By comparison of the accuracy generated by three different models. We can determine the SVM is the best one, and will be used to predict 20 different test cases.

```{r}
library(kernlab)
Data <- predict(preProcValue, cleanTestData[, -53])
pred <- predict(svmFit, Data)
pred
```
By doing the 2nd assignment. The predicted value given by SVM can give 100% accuracy for the 20 test cases.
